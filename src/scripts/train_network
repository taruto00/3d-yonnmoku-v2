# -*- coding: utf-8 -*-
"""train_network

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FVnUcLhf3foCpPUhqxyD5EhLgSsx9MZ0
"""

# =====================================================
# train_network_bitboard.py
#   - ビットボード履歴で Dual Net を再学習
# =====================================================
from __future__ import annotations
from dataclasses import dataclass
from typing import List
from pathlib import Path
import math, numpy as np
import pickle, os, sys
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import LearningRateScheduler, LambdaCallback, EarlyStopping
from tensorflow.keras import backend as K

from dual_network import DN_INPUT_SHAPE         # (4,4,14)
from game import (
    COL_MASKS,
    LINES76,
    dr2_plane,
    ketsu_even_odd_filtered,
    drp_plane,
)
RN_EPOCHS = 80          # エポック数
SIZE      = DN_INPUT_SHAPE[0]                   # 4

# --- ダブルリーチ検出 --------------------------------

def detect_double_reach(my: int, opp: int) -> Tuple[bool, bool]:
    """
    戻り値: (is_DR1, is_DR2)
      DR1 = 異なる列に決勝点が 2 つ以上
      DR2 = 同一列に連続する高さの決勝点が 2 つ以上
    """
    # 決勝点ビットを集める
    occ = my | opp
    ketsu = 0
    for line in LINES76:
        if line & opp: continue
        if (line & my).bit_count() == 3:
            ketsu |= line & ~occ

    cnt_cols = 0
    dr2_found = False
    for col in COL_MASKS:
        col_k = ketsu & col
        if not col_k:
            continue
        # DR1 判定
        cnt_cols += 1
        # DR2 判定: 連続高さチェック
        zs = [(idx // 16) for idx in range(64) if col_k & (1 << idx)]
        zs.sort()
        for z1, z2 in zip(zs, zs[1:]):
            if z2 == z1 + 1:
                dr2_found = True
                break
    return (cnt_cols >= 2, dr2_found)


# --- 対称変換ヘルパ ----------------------------------------
def rotate_planes(planes, k):
    """(4,4,8) を xy 平面で k×90° 回転"""
    return np.rot90(planes, k=k, axes=(0,1))  # y,x を回転

def flip_planes(planes):
    """左右反転"""
    return planes[:, ::-1, :]                # x 反転

def transform_policy(policy16, k_rot, flip):
    mat = np.arange(16).reshape(4,4)
    mat = np.rot90(mat, k=k_rot)
    if flip:
        mat = mat[:, ::-1]
    mapping = mat.ravel()                        # numpy array of 16 ints

    arr = np.asarray(policy16, dtype=np.float32) # ensure numpy indexing
    return arr[mapping]                          # returns length-16 array


# -----------------------------------------------------
# 盤面 64bit → (4,4,14) テンソル  *ベクトル化版*
# -----------------------------------------------------
def bitboards_to_tensor_batch(pieces_arr, enemy_arr):
    N = len(pieces_arr)
    mine_bits  = np.unpackbits(pieces_arr.view(np.uint8).reshape(N,8), bitorder='little')
    yours_bits = np.unpackbits(enemy_arr .view(np.uint8).reshape(N,8), bitorder='little')

    mine  = mine_bits .reshape(N, SIZE, SIZE, SIZE).transpose(0,2,3,1)   # (N,4,4,4)
    yours = yours_bits.reshape(N, SIZE, SIZE, SIZE).transpose(0,2,3,1)

    planes = np.zeros((N, SIZE, SIZE, 14), np.float32)   # ← 14 ch

    planes[..., :SIZE]        = mine                    # ch 0-3
    planes[..., SIZE:2*SIZE]  = yours                   # ch 4-7

    # --- 追加 6 ch --------------------------------------------------
    for i in range(N):
        my, op = int(pieces_arr[i]), int(enemy_arr[i])

        # DR-2 プレーン
        planes[i, ...,  8] = dr2_plane(my, op)          # 自分
        planes[i, ...,  9] = dr2_plane(op, my)          # 相手

        # 偶/奇 決勝点（真下が敵の決勝点ではない条件付き）
        even, odd          = ketsu_even_odd_filtered(my, op)
        planes[i, ..., 10] = even
        planes[i, ..., 11] = odd

        # DR-P 2 ch
        planes[i, ..., 12] = drp_plane(my, op)   # 自分
        planes[i, ..., 13] = drp_plane(op, my)   # 相手

    return planes


# -----------------------------------------------------
# リプレイ読み込み
# -----------------------------------------------------

def load_history(buffer_size: int = 100000):
    # --- ① 自己対戦履歴を最新→過去へ読み込み ---
    files = sorted(Path("./data").glob("*.history"), reverse=True)
    merged = []
    for p in files:
        merged.extend(pickle.load(p.open("rb")))
        if len(merged) >= buffer_size:
            break

    # --- ② 追加教師（forced_history）があれば挿入 ---
    #forced_path = Path("./data/forced_parallel.history")
    #if forced_path.exists():
    #    forced_data = pickle.load(forced_path.open("rb"))
    #    merged.extend(forced_data)

    # --- ③ ランダムシャッフル ---
    rng = np.random.default_rng()      # 乱数シード固定しない方が多様化
    rng.shuffle(merged)

    # --- ④ 上限でトリムして返す ---
    return merged[:buffer_size]



# -----------------------------------------------------
# 学習ループ
# -----------------------------------------------------
def train_network():
    history = load_history()

    pieces_int, enemy_int = zip(*[h[0] for h in history])
    y_policies, y_values  = zip(*[(h[1], h[2]) for h in history])

    pieces_arr = np.frombuffer(np.array(pieces_int, dtype=np.uint64), dtype=np.uint64)
    enemy_arr  = np.frombuffer(np.array(enemy_int,  dtype=np.uint64), dtype=np.uint64)


    # --- 入力テンソル (N,4,4,14)
    xs = bitboards_to_tensor_batch(pieces_arr, enemy_arr)

    # ---------- DR-2 ラベル生成 ---------------------------------
    y_dr2 = np.array([
        float(detect_double_reach(int(m), int(o))[1])
        for m, o in zip(pieces_arr, enemy_arr)
    ], dtype=np.float32)

    # ── Augmentation ──────────────────────────────────────
    aug_xs, aug_p, aug_v, aug_d = [], [], [], []
    for planes, pol, val, d2 in zip(xs, y_policies, y_values, y_dr2):
        for k in range(4):
            for flip in (False, True):
                p_tr = transform_policy(pol, k, flip)
                pln  = rotate_planes(planes, k)
                if flip: pln = flip_planes(pln)
                aug_xs.append(pln)
                aug_p.append(p_tr)
                aug_v.append(val)
                aug_d.append(d2)          # ← 追加
    # ------------------------------------------------------

    xs         = np.asarray(aug_xs, dtype=np.float32)
    y_policies = np.asarray(aug_p,  dtype=np.float32)
    y_values   = np.asarray(aug_v,  dtype=np.float32)
    y_dr2      = np.asarray(aug_d,  dtype=np.float32).reshape(-1,1)

    print(f"DEBUG  augmented N = {len(xs)}")
    # =============================================================

    # --- チェック①：policy 正規化と value 分布 ---
    import numpy as _np
    # policy がすべて 1 に正規化されているか
    print("DEBUG policy_sum≈1 ?", _np.allclose(_np.sum(y_policies, axis=1), 1.0))
    # value ラベルに +1, -1, 0 がちゃんと混ざっているか
    print("DEBUG value distribution", _np.unique(y_values, return_counts=True))

    nonzero_ratio = (xs.sum(axis=(1,2,3)) > 0).mean()
    avg_bits      = xs.sum() / len(xs)

    print(f"DEBUG  non-zero サンプル率 = {nonzero_ratio:.3f}")
    print(f"DEBUG  1サンプル当たり立っているビット数 = {avg_bits:.1f}")

    # --- データ拡張後 かつ デバッグチェック直後 に挿入 ---
    N = xs.shape[0]
    idx = np.arange(N)
    np.random.shuffle(idx)

    # 10% を検証用に
    val_n     = int(N * 0.1)
    val_idx   = idx[:val_n]
    train_idx = idx[val_n:]

    # 訓練／検証データに分割
    xs_train     = xs[train_idx]
    y_p_train    = y_policies[train_idx]
    y_v_train    = y_values[train_idx]

    xs_val       = xs[val_idx]
    y_p_val      = y_policies[val_idx]
    y_v_val      = y_values[val_idx]

    y_dr2_train = y_dr2[train_idx]
    y_dr2_val   = y_dr2[val_idx]


    print(f"DEBUG  train N = {len(train_idx)},  val N = {len(val_idx)}")

    # --- 出力ラベル
    y_policies = np.asarray(y_policies, dtype=np.float32)
    y_values   = np.asarray(y_values,   dtype=np.float32)

    # --- モデル取得
    if not Path("./model/best_v2.h5").exists():
        from dual_network import dual_network
        dual_network()
    model = load_model("./model/best_v2.h5", compile=False)

    losses = [
        "categorical_crossentropy",    # policy
        "mse",                         # value
        "binary_crossentropy",         # dr2
    ]

    model.compile(
        loss=losses,
        loss_weights=[1.0, 1.0, 2.0],  # ← 最初は DR-2 を強調
        optimizer="adam",
    )

    # --- 学習率スケジューラ
    #def step_decay(epoch):
        #return 0.00025 if epoch >= 80 else 0.0005 if epoch >= 50 else 0.0003 #エポック数を100→20に変更

    #def step_decay(epoch):
     #   return 0.0001 if epoch>=15 else 0.0002 if epoch>=8 else 0.0003
    #lr_sched = LearningRateScheduler(step_decay, verbose=0)

    #def step_decay(epoch):
     # if epoch < 10:  return 5e-4   # ← 最初を少し下げる
     # if epoch < 20:  return 2.5e-4
     # return 1e-4

    #def step_decay(epoch):
     #   """epoch に応じて学習率を 4 段階で減衰させる"""
     #   if   epoch < 10: return 5e-4        # 0– 9 epoch
     #   elif epoch < 20: return 2.5e-4      # 10–19 epoch
     #   elif epoch < 30: return 1e-4        # 20–29 epoch
     #   else:            return 5e-5        # 30–39 epoch
    #def step_decay(epoch):
     #   """epoch に応じて学習率を 4 段階で減衰させる"""
     #   if   epoch < 20: return 5e-4        # 0– 9 epoch
     #   elif epoch < 40: return 2.5e-4      # 10–19 epoch
     #   elif epoch < 60: return 1e-4        # 20–29 epoch
     #   else:            return 5e-5        # 30–39 epoch

    def step_decay(epoch):
        """epoch に応じて学習率を 4 段階で減衰させる"""
        if   epoch < 16: return 5e-4        # 0– 9 epoch
        elif epoch < 36: return 2.5e-4      # 10–19 epoch
        elif epoch < 56: return 1e-4        # 20–29 epoch
        else:            return 5e-5        # 30–39 epoch

    lr_sched = LearningRateScheduler(step_decay, verbose=0)


    # --- 進捗表示
    print_cb = LambdaCallback(
        on_epoch_begin=lambda epoch, logs:
            print(f"\rTrain {epoch+1}/{RN_EPOCHS}", end="")
    )
    # 追加: EarlyStopping（val_loss が 5 エポック改善しなければ終了）
    es = EarlyStopping(
        monitor="val_loss",   # 検証損失を監視
        patience=10,           # 5 エポック改善がなければ
        restore_best_weights=True,
        verbose=1
    )
    # --- モデル学習 ---
    history_obj = model.fit(
        xs_train,
        [y_p_train, y_v_train, y_dr2_train],
        validation_data=(xs_val, [y_p_val, y_v_val, y_dr2_val]),
        batch_size = 128,
        epochs     = RN_EPOCHS,
        shuffle    = True,
        verbose    = 0,
        callbacks  = [lr_sched, print_cb, es]

    )
    print()  # 改行

    # --- チェック②：学習損失の推移 ---
    losses = history_obj.history["loss"]
    print(f"DEBUG loss: start={losses[0]:.3f}, end={losses[-1]:.3f}")

    # --- 保存
    os.makedirs("./model", exist_ok=True)
    model.save("./model/latest_v2.h5")
    K.clear_session()
